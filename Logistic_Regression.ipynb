{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjXP5sBdBUeYlvpT7/uP9m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "_JPb6_AjReKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQ8604TdRVUP"
      },
      "outputs": [],
      "source": [
        "x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\n",
        "y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self):\n",
        "    self.w = 0\n",
        "    self.b = 0\n",
        "\n",
        "  def predict(self, X: np.ndarray):\n",
        "    return self.sigmoid(X.dot(self.w) + self.b)\n",
        "\n",
        "  def sigmoid(self,prediction: np.ndarray):\n",
        "    z = 1/(1+np.exp(-prediction))\n",
        "    return z\n",
        "\n",
        "  def computeCost(self, X: np.ndarray, y:np.ndarray):\n",
        "    m = X.shape[0] #no of rows of X\n",
        "    prediction = X.dot(self.w) + self.b #making prediction with weights\n",
        "    prediction = self.sigmoid(prediction) #maping the prediction to 0 or 1\n",
        "    cost = np.sum((y*np.log(prediction))+ ((1-y)*np.log(1-prediction))) #computing the losses\n",
        "    cost = (1/m) * -cost  #computing the total cost\n",
        "    return cost\n",
        "\n",
        "  def fit(self, X: np.ndarray, y:np.ndarray, alpha:int, epochs: int):\n",
        "    y = y.reshape(-1,1)\n",
        "    m = X.shape[1]\n",
        "    self.w = np.zeros((m,1))\n",
        "\n",
        "    for i in range(epochs):\n",
        "      cost = self.computeCost(X,y)\n",
        "      if i%100 ==0:\n",
        "        print(f\"Epoch: {i+1}, Loss: {cost}\")\n",
        "      prediction = self.predict(X)\n",
        "      error = prediction - y\n",
        "      gradient = np.dot(X.transpose(),error)\n",
        "      self.w -= alpha * 1/m * gradient\n",
        "      self.b -= alpha * 1/m * np.sum(error)\n",
        "\n",
        "    return (self.w,self.b)"
      ],
      "metadata": {
        "id": "5Yp8iMxSR5DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y = np.array([0, 0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "E9RHNFx9FyCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y"
      ],
      "metadata": {
        "id": "w4jW20eBG8nV",
        "outputId": "e837cbad-d948-4dd7-d28f-d763d35d2499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.5, 1.5],\n",
              "        [1. , 1. ],\n",
              "        [1.5, 0.5],\n",
              "        [3. , 0.5],\n",
              "        [2. , 2. ],\n",
              "        [1. , 2.5]]),\n",
              " array([[0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()"
      ],
      "metadata": {
        "id": "r6GTlRHFFzEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.reshape(-1,1)"
      ],
      "metadata": {
        "id": "aNAbFFnyGCOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.fit(X,y,1e-7,10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFCJ1h4rF1w-",
        "outputId": "f08aae0b-b5c2-4447-8455-831b532a0af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.6931471805599452\n",
            "Epoch: 101, Loss: 0.693144472314579\n",
            "Epoch: 201, Loss: 0.6931417642460218\n",
            "Epoch: 301, Loss: 0.6931390563542594\n",
            "Epoch: 401, Loss: 0.6931363486392776\n",
            "Epoch: 501, Loss: 0.693133641101063\n",
            "Epoch: 601, Loss: 0.693130933739601\n",
            "Epoch: 701, Loss: 0.6931282265548777\n",
            "Epoch: 801, Loss: 0.6931255195468788\n",
            "Epoch: 901, Loss: 0.6931228127155908\n",
            "Epoch: 1001, Loss: 0.6931201060609993\n",
            "Epoch: 1101, Loss: 0.6931173995830902\n",
            "Epoch: 1201, Loss: 0.6931146932818498\n",
            "Epoch: 1301, Loss: 0.6931119871572637\n",
            "Epoch: 1401, Loss: 0.693109281209318\n",
            "Epoch: 1501, Loss: 0.6931065754379988\n",
            "Epoch: 1601, Loss: 0.6931038698432919\n",
            "Epoch: 1701, Loss: 0.6931011644251832\n",
            "Epoch: 1801, Loss: 0.6930984591836588\n",
            "Epoch: 1901, Loss: 0.6930957541187046\n",
            "Epoch: 2001, Loss: 0.6930930492303068\n",
            "Epoch: 2101, Loss: 0.693090344518451\n",
            "Epoch: 2201, Loss: 0.6930876399831234\n",
            "Epoch: 2301, Loss: 0.69308493562431\n",
            "Epoch: 2401, Loss: 0.6930822314419967\n",
            "Epoch: 2501, Loss: 0.6930795274361692\n",
            "Epoch: 2601, Loss: 0.6930768236068139\n",
            "Epoch: 2701, Loss: 0.6930741199539165\n",
            "Epoch: 2801, Loss: 0.6930714164774634\n",
            "Epoch: 2901, Loss: 0.69306871317744\n",
            "Epoch: 3001, Loss: 0.6930660100538326\n",
            "Epoch: 3101, Loss: 0.6930633071066272\n",
            "Epoch: 3201, Loss: 0.6930606043358096\n",
            "Epoch: 3301, Loss: 0.6930579017413661\n",
            "Epoch: 3401, Loss: 0.6930551993232823\n",
            "Epoch: 3501, Loss: 0.6930524970815443\n",
            "Epoch: 3601, Loss: 0.6930497950161385\n",
            "Epoch: 3701, Loss: 0.6930470931270502\n",
            "Epoch: 3801, Loss: 0.6930443914142655\n",
            "Epoch: 3901, Loss: 0.6930416898777711\n",
            "Epoch: 4001, Loss: 0.6930389885175523\n",
            "Epoch: 4101, Loss: 0.6930362873335953\n",
            "Epoch: 4201, Loss: 0.6930335863258863\n",
            "Epoch: 4301, Loss: 0.6930308854944107\n",
            "Epoch: 4401, Loss: 0.6930281848391551\n",
            "Epoch: 4501, Loss: 0.6930254843601051\n",
            "Epoch: 4601, Loss: 0.6930227840572469\n",
            "Epoch: 4701, Loss: 0.6930200839305668\n",
            "Epoch: 4801, Loss: 0.69301738398005\n",
            "Epoch: 4901, Loss: 0.6930146842056832\n",
            "Epoch: 5001, Loss: 0.6930119846074523\n",
            "Epoch: 5101, Loss: 0.6930092851853429\n",
            "Epoch: 5201, Loss: 0.6930065859393413\n",
            "Epoch: 5301, Loss: 0.6930038868694336\n",
            "Epoch: 5401, Loss: 0.6930011879756056\n",
            "Epoch: 5501, Loss: 0.6929984892578434\n",
            "Epoch: 5601, Loss: 0.6929957907161328\n",
            "Epoch: 5701, Loss: 0.6929930923504602\n",
            "Epoch: 5801, Loss: 0.6929903941608113\n",
            "Epoch: 5901, Loss: 0.6929876961471725\n",
            "Epoch: 6001, Loss: 0.6929849983095293\n",
            "Epoch: 6101, Loss: 0.6929823006478679\n",
            "Epoch: 6201, Loss: 0.6929796031621744\n",
            "Epoch: 6301, Loss: 0.6929769058524349\n",
            "Epoch: 6401, Loss: 0.692974208718635\n",
            "Epoch: 6501, Loss: 0.6929715117607614\n",
            "Epoch: 6601, Loss: 0.6929688149787994\n",
            "Epoch: 6701, Loss: 0.6929661183727355\n",
            "Epoch: 6801, Loss: 0.6929634219425554\n",
            "Epoch: 6901, Loss: 0.6929607256882455\n",
            "Epoch: 7001, Loss: 0.6929580296097916\n",
            "Epoch: 7101, Loss: 0.6929553337071797\n",
            "Epoch: 7201, Loss: 0.6929526379803956\n",
            "Epoch: 7301, Loss: 0.6929499424294259\n",
            "Epoch: 7401, Loss: 0.692947247054256\n",
            "Epoch: 7501, Loss: 0.6929445518548724\n",
            "Epoch: 7601, Loss: 0.692941856831261\n",
            "Epoch: 7701, Loss: 0.6929391619834078\n",
            "Epoch: 7801, Loss: 0.6929364673112988\n",
            "Epoch: 7901, Loss: 0.6929337728149201\n",
            "Epoch: 8001, Loss: 0.6929310784942576\n",
            "Epoch: 8101, Loss: 0.6929283843492975\n",
            "Epoch: 8201, Loss: 0.6929256903800255\n",
            "Epoch: 8301, Loss: 0.6929229965864281\n",
            "Epoch: 8401, Loss: 0.6929203029684912\n",
            "Epoch: 8501, Loss: 0.6929176095262006\n",
            "Epoch: 8601, Loss: 0.6929149162595426\n",
            "Epoch: 8701, Loss: 0.6929122231685032\n",
            "Epoch: 8801, Loss: 0.6929095302530683\n",
            "Epoch: 8901, Loss: 0.6929068375132241\n",
            "Epoch: 9001, Loss: 0.6929041449489566\n",
            "Epoch: 9101, Loss: 0.6929014525602517\n",
            "Epoch: 9201, Loss: 0.6928987603470957\n",
            "Epoch: 9301, Loss: 0.6928960683094745\n",
            "Epoch: 9401, Loss: 0.6928933764473741\n",
            "Epoch: 9501, Loss: 0.6928906847607805\n",
            "Epoch: 9601, Loss: 0.6928879932496802\n",
            "Epoch: 9701, Loss: 0.6928853019140587\n",
            "Epoch: 9801, Loss: 0.6928826107539023\n",
            "Epoch: 9901, Loss: 0.692879919769197\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.00074885],\n",
              "        [0.00049907]]),\n",
              " -6.709003463317607e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}